{
   // Experiment configuration.
   "experiment" : {

      // Root directory to save results.
      "logdir" : "./log",

      // Random number seed. Don't forget to change this for multiple runs!
      "seed" : 0
   },
   //pinn
   "pinn":  {
      //task param
      "use_pinn":true,
      "use_variance":false,
      // "iter_num":2,
      //network param
      // "number_layer":8,
      // "input_dim":2,
      // "n_hidden":32,
      // "out_dim":1,
      "activation":"sin",
      "local_sample":false,
      // "coef_pde":1,
      "pinn_epoch": 1000,
      "duration":500,
      "lr":0.001,
      // data
      // "data_ratio":0.13,
      // "noise":0.1,
      // "coll_data":20000,
      "generation_type": "AD"
   },
   
   // Task-specific hyperparameters. See task-specific configs (e.g.
   // config_regression.json) for more info.
   "task" : {

      // Choice of ["PDE"].
      "task_type" : null,
      // "cut_ratio":0.1,
      "use_torch":false,
      // "data_noise_level":0.1,
      // "sym_true_input": "add_t,mul_t,u,diff_t,u,x1,add_t,diff2_t,u,x1,diff4_t,u,x1",
      // "data_amount":1,
   },

   // Hyperparameters related to the main training loop.
   "training" : {

      // These parameters control the length of the run. Specify exactly one of
      // [n_epochs, n_samples]. The other must be null.
      "n_epochs" : null,
      "n_samples" : 2000000,
      "batch_size" : 500,

      // To use the risk-seeking policy gradient, set epsilon < 1.0 and
      // baseline="R_e"
      "epsilon" : 0.05,
      "baseline" : "R_e",

      // Control variate parameters for vanilla policy gradient. If risk-seeking
      // is used, these have no effect.
      "alpha" : 0.5,
      "b_jumpstart" : false,

      // Number of cores to use when evaluating a batch of rewards. For batch
      // runs using run.py and --runs > 1, this will be overridden to 1. For
      // single runs, recommended to set this to as many cores as you can use!
      "n_cores_batch" : 1,

      // The complexity measure is only used to compute a Pareto front. It does
      // not affect the optimization.
      "complexity" : "token",
      
      // Default terms for equations as prior info
      "default_terms":[],

      // The constant optimizer used to optimized each "const" token.
      "const_optimizer" : "scipy",
      "const_params" : {},
      "verbose" : true,

      // Debug level
      "debug" : 1,

      // Whether to stop early if success condition is met
      "early_stopping" : true,

      // Size of the "hall of fame" (top performers during training) to save.
      "hof" : 100,

      // EXPERIMENTAL: Hyperparameters related to utilizing a memory buffer.
      "use_memory" : false,
      "memory_capacity" : 1e3,
      "warm_start" : null,
      "memory_threshold" : null,

      // Parameters to control what outputs to save.
      "save_all_epoch" : false,
      "save_summary" : true,
      "save_positional_entropy" : false,
      "save_pareto_front" : true,
      "save_cache" : false,
      "save_cache_r_min" : 0.9,
      "save_freq" : 1,
      "save_token_count" : false,

      // Parameters new
      "remove_same":false,
   },
   // The State Manager defines the inputs to the Controller
   "state_manager": {
         "type" : "hierarchical",
         // Observation hyperparameters
         "observe_action" : false,
         "observe_parent" : true,
         "observe_sibling" : true,
         "observe_dangling" : false,
         "embedding" : false,
         "embedding_size" : 8
   },
   // Hyperparameters related to the RNN distribution over objects.
   "controller" : {
      // Maximum sequence length.
      "max_length" : 256,

      // RNN architectural hyperparameters.
      "cell" : "lstm",
      "num_layers" : 1,
      "num_units" : 32,
      "initializer" : "zeros",

      // Optimizer hyperparameters.
      "learning_rate" : 0.001,
      "optimizer" : "adam",

      // Entropy regularizer hyperparameters.
      "entropy_weight" : 0.005,
      "entropy_gamma" : 1.0,

      // EXPERIMENTAL: Priority queue training hyperparameters.
      "pqt" : false,
      "pqt_k" : 10,
      "pqt_batch_size" : 1,
      "pqt_weight" : 200.0,
      "pqt_use_pg" : false,

      // Whether to compute TensorBoard summaries.
      "summary" : true,
      "attention": false,
      "atten_len":20
   },

   // Hyperparameters related to genetic programming hybrid methods.
   "gp_meld" : {
      "run_gp_meld" : false,
      "verbose" : false,
      "generations" : 20,
      "p_crossover" : 0.5,
      "p_mutate" : 0.5,
      "tournament_size" : 5,
      "train_n" : 50,
      "mutate_tree_max" : 3
   },

   // Hyperparameters related to including in situ priors and constraints. Each
   // prior must explicitly be turned "on" or it will not be used.
   "prior": {
      // Whether to count number of constraints (useful diagnostic but has some overhead)
      "count_constraints" : false,

      // Generic constraint that [targets] cannot be the [relationship] of
      // [effectors]. See prior.py for supported relationships.
      "relational" : {
         "targets" : [],
         "effectors" : [],
         "relationship" : null,
         "on" : false
      },

      // Constrains expression length to fall between min_ and max_, inclusive.
      "length" : {
         "min_" : 4,
         "max_" : 30,
         "on" : false
      },

      // Constraints "const" to appear at most max_ times.
      "repeat" : {
         "tokens" : "const",
         "min_" : null,
         "max_" : 3,
         "on" : false
      },

      // Prevents consecutive inverse unary operators, e.g. log(exp(x)).
      "inverse" : {
         "on" : false
      },

      // Prevents nested trig operators (e.g. sin(1 + cos(x))).
      "trig" : {
         "on" : false
      },

      // Prevents "const" from being the only child, e.g. sin(const) or
      // add(const, const).
      "const" : {
         "on" : false
      },

      // Prevents expressions with no input variables, e.g. sin(1.0 + const).
      "no_inputs" : {
         "on" : false
      },

      "laplace_child":{
         "on": false
      },

      // Uniform arity prior: the prior probabilities over arities is uniform.
      "uniform_arity" : {
         "on" : false
      },

      // Soft length prior: expressions are discouraged to have length far from
      // loc.
      "soft_length" : {
         "loc" : 10,
         "scale" : 5,
         "on" : false
      },

      // EXPERIMENTAL: Language model to use as a prior.
      // "language_model" : {
      //    "weight" : null,
      //    "on" : false
      // }

   },

   // Postprocessing hyperparameters.
   "postprocess" : {
      "show_count" : 5,
      "save_plots" : true
   }
}
